{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemEval-2018 Task 1: Affect in Tweets (AIT-2018)\n",
    "\n",
    "## An emotion intensity ordinal classification task\n",
    "\n",
    "*Joana Ferreira |\n",
    "joanaferreira0011@gmail.com |\n",
    "Faculty of Engineering, University of Porto, R. Dr. Roberto Frias, 4200-465 Porto, Portugal *\n",
    "\n",
    "### Abstract:\n",
    "This notebook presents a solution for the SemEval-2018 Task 1: Affect in Tweets (AIT-2018). Given a tweet and an emotion (sadness, anger, fear, joy), the model should output an intensity classification (0, 1, 2, 3). To do this, two different approaches were implemented: the first one using a knowledge based preprocessing the second one using word embeddings (BERT). Finally, several models were used and compared to do the classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "Given a tweet and an emotion (sadness, anger, fear or joy), the proposed task is to classify it in one of four classes given its intensity: (0:no,\n",
    "1: low, 2: moderate, 3: high). The task was solved only for the English language. \n",
    "\n",
    "This task was proposed in (Mohammad et al.,2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Description of the dataset\n",
    "There are four training and test sets of labeled data â€“ one for each emotion. The data creation is described in (Mohammad and Kiritchenko, 2018).\n",
    "\n",
    "The training and test sets were merged together and then split (20/80) in order to increase the size of the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "li = []\n",
    "\n",
    "#Change this the correct path of the desired dataset for a specific emotion\n",
    "all_files= {'../datasets/2018-EI-oc-En-anger-dev.txt', '../datasets/EI-oc-En-anger-train.txt'}\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", header=None, skiprows=1)\n",
    "    li.append(df)\n",
    "\n",
    "dataset = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "dataset.columns = ['date', 'text', 'emotion', 'level']\n",
    "dataset= dataset.sample(frac=1)\n",
    "print (dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Approach \n",
    "\n",
    "Two alternatives were implemented for preprocessing: a knowledge based approach (section 3.1.1) and one using word embeddings (BERT) (section 3.1.2). After the preprocessing, both data were run for the same models: Naive Bayes, SVM, Logistic regression, Multi-layer Perceptron classifier, Perceptron, Decision Tree and Random Forest.\n",
    "\n",
    "All of the models and preprocessing were run for each emotion seperately, as the emotion is an input of the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Preprocessing using a knowledge based approach\n",
    "In this approach, the following processes were implemented:\n",
    "* **Tokenization**: tokens were created using Python String split() method. Other tokenization methods were tested such as NLTK word_tokenize(), but no significant improvements were observed.\n",
    "* **Lower case**: tokens were converted to lower case\n",
    "* **Lemmatization**: using NLTK WordNetLemmatizer\n",
    "* **User**: All user mentions were substituted with '@user'\n",
    "* **Emojis**: All emojis were converted to keywords, using the 'emoji' library\n",
    "* **TF-IDF**: SkLearn's TfidfVectorizer was used to perform TF-IDF\n",
    "* **Bag of Words (BoW)**: Bag of words was used but eventually substitute with TF-IDF, because of the better overall accuracy\n",
    "\n",
    "\n",
    "*Skip to section 3.1.2. and do not run the following 3 cells if you want to see the results from using word embeddings (BERT).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "corpus = []\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    tweet = row['text']\n",
    "\n",
    "    #handle users\n",
    "    tweet = re.sub('@.*', '@user', tweet) \n",
    "\n",
    "    \n",
    "    \n",
    "    tweet = tweet.tolower().split()\n",
    "    #tweet = nltk.word_tokenize(tweet) - can be used but a few changes need to be done in the following lines because the data structures will change\n",
    "\n",
    "    # lemmatization and stop word removal\n",
    "    tweet = ' '.join([lemmatizer.lemmatize(w) for w in tweet if not w in set(stopwords.words('english'))])\n",
    "    \n",
    "\n",
    "    #tweet = nlp(tweet) # run annotation over a sentence\n",
    "    \n",
    "    \n",
    "    #emojis\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    \n",
    "    corpus.append(tweet)\n",
    "\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tfidf_vectorizer_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X = tfidf_vectorizer_vectors.toarray()\n",
    "y = []\n",
    "for index, row in dataset.iterrows():\n",
    "    y.append(int(row['level'][0]))\n",
    "\n",
    "    #Uncomment the following lines and comment the one before to reduce the number of classes to a binary classification\n",
    "    '''\n",
    "    if(int(row['level'][0])>=2):\n",
    "        y.append(1) \n",
    "    else:\n",
    "        y.append(0)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "y = np.array(y)\n",
    "\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Preprocessing using BERT\n",
    "In this approach, word embedding was used to preprocess the data. For this, BERT was chosen and the library *bert_embedding* was used because of its simplicity. The model was pre-trained in the following dataset: *book_corpus_wiki_en_cased*.\n",
    "\n",
    "*Skip to section 3.2 and do not run the following 3 cells if you want to see the results from using the knowledge based approach described in 3.1.1.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from bert_embedding import BertEmbedding\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import mxnet as mx\n",
    "\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    tweet = row['text']\n",
    "    corpus.append(tweet)\n",
    "\n",
    "bert = BertEmbedding(model='bert_24_1024_16', dataset_name='book_corpus_wiki_en_cased')\n",
    "results = bert(corpus)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "averaged = []\n",
    "for sent in results:\n",
    "    averaged.append(np.mean(sent[1], axis = 0, dtype=np.float64))\n",
    "\n",
    "corpus=averaged\n",
    "X=np.array(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "y = []\n",
    "for index, row in dataset.iterrows():\n",
    "    y.append(int(row['level'][0]))\n",
    "    \n",
    "    #Uncomment the following lines and comment the one before to reduce the number of classes to a binary classification\n",
    "    '''\n",
    "    if(int(row['level'][0])>=2):\n",
    "        y.append(1) \n",
    "    else:\n",
    "        y.append(0)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "y = np.array(y)\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Classification\n",
    "After the preprocessing, the data was run for the following models (All from SkLearn). \n",
    "* Naive Bayes \n",
    "* Support Vector Machine(SVM)\n",
    "* Logistic regression\n",
    "* Multi-layer Perceptron classifier\n",
    "* Perceptron\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "\n",
    "To evaluate and compare them, Accuracy, Precision, Recall and F1 were measured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGDC classifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "classifier = SGDClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifier = MLPClassifier(random_state=1, max_iter=300)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "classifier = Perceptron() \n",
    "classifier.fit(X_train, y_train) \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred)) \n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted')) \n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted')) \n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Experimental evaluation\n",
    "\n",
    "#### 4.1 Accuracy\n",
    "\n",
    "The following values for the accuracy were obtain as the median of 3 runs for each model, using Sklearn's *accuracy_score*.\n",
    "\n",
    "#### 4.1.1 Joy \n",
    "\n",
    "| Models | Naive Bayes | SVM | Logistic Regression | MLPClassifier | Perceptron | Decision tree | Random forest |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Knowledge | 0.346 | **0.419** | 0.408 | 0.369 | 0.397 | 0.390 | 0.408 |\n",
    "| BERT | 0.471 | 0.440 | 0.424 | 0.458 | 0.400 | 0.372 | **0.476** |\n",
    "\n",
    "#### 4.1.2 Sadness\n",
    "\n",
    "| Models | Naive Bayes | SVM | Logistic Regression | MLPClassifier | Perceptron | Decision tree | Random forest |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Knowledge | 0.326 | 0.436 | 0.471 | 0.443 | **0.474** | 0.463 | **0.474** |\n",
    "| BERT | 0.409 | **0.489** | 0.437 | 0.472 | 0.409 | 0.319 | 0.432 |\n",
    "\n",
    "#### 4.1.3 Anger\n",
    "\n",
    "| Models | Naive Bayes | SVM | Logistic Regression | MLPClassifier | Perceptron | Decision tree | Random forest |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Knowledge | 0.284 | 0.430 | **0.447** | 0.411 | 0.406 | 0.409 | **0.447** |\n",
    "| BERT | 0.411 | **0.480** | 0.450 | 0.483 | 0.428 | 0.378 | 0.443 |\n",
    "\n",
    "#### 4.1.4 Fear\n",
    "\n",
    "| Models | Naive Bayes | SVM | Logistic Regression | MLPClassifier | Perceptron | Decision tree | Random forest |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Knowledge | 0.319 | 0.677 | 0.674 | 0.633 | 0.686 | 0.640 | **0.691** |\n",
    "| BERT | 0.539 | **0.681** | 0.621 | 0.671 | 0.639 | 0.533 | 0.665 |\n",
    "\n",
    "#### 4.2 Confusion Matrices\n",
    "\n",
    "For each run 4 metrics were calculated using Sklearn: accuracy, precision, recall and F1.\n",
    " \n",
    "Most confusion matrices obtained showed that when the prediction was wrong, most algorithms actually predicted one of the closests classes to the expected one. For instance, when expected class 0, if the prediction was wrong the algorithm would more likely have predicted 1, than 3 or 4.\n",
    "\n",
    "An example of one matrix can be found here:\n",
    "\n",
    "| Class | 0 | 1 | 2 | 3 |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 0 | 56 | 36 | 16 | 19 | \n",
    "| 1 | 30 | 15 | 25 | 7 | \n",
    "| 2 | 21 | 31 |36 | 29 | \n",
    "| 3 | 12 | 9 | 11 | 65 | \n",
    "\n",
    "Accuracy:  0.41148325358851673 | \n",
    "Precision:  0.4135462119735973 | \n",
    "Recall:  0.41148325358851673 | \n",
    "F1:  0.40855124453720665 |\n",
    "\n",
    "*Confusion matrix and metrics for one run - emotion Anger: BERT with Naive Bayes*\n",
    "\n",
    "\n",
    "#### 4.3 Fewer classes\n",
    "\n",
    "Given the observations taken from the confusion matrices, the algortithms were run again but in for a binary classification. \n",
    "* Classes 0 and 1 converged to Class 0\n",
    "* Classes 2 and 3 converged to Class 1\n",
    "\n",
    "Given this simplification, most accuracies increased by around 20%. This corroborates the idea that the models have some difficulty distinguishing classes that are closer to each other.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusions\n",
    "\n",
    "After analysing the results, we conclude that the best results were mostly obtained by using BERT. Reggarding the models, SVM and Random Forest tend to outperform the rest for almost all the tasks. \n",
    "\n",
    "In the future, other BERT algorithms and embeddings should be tested and a convulutional neural network (CNN) should also be implemented. Fine tuning the models even further with either types of preprocessing should also contribute to a better overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Priban, Pavel & Hercig, TomÃ¡Å¡ & Lenc, Ladislav. (2018). UWB at SemEval-2018 Task 1: Emotion Intensity Detection in Tweets. 133-140. 10.18653/v1/S18-1018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('python37': venv)",
   "language": "python",
   "name": "python36964bitpython37venvb1b630360d0f4ea08bfe0f6fcf349387"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

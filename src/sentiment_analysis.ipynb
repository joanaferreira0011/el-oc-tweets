{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemEval-2018 Task 1: Affect in Tweets (AIT-2018)\n",
    "\n",
    "## An emotion intensity ordinal classification task\n",
    "\n",
    "*Joana Ferreira |\n",
    "joanaferreira0011@gmail.com |\n",
    "Faculty of Engineering, University of Porto, R. Dr. Roberto Frias, 4200-465 Porto, Portugal *\n",
    "\n",
    "### Abstract:\n",
    "This notebook presents a solution for the SemEval-2018 Task 1: Affect in Tweets (AIT-2018). Given a tweet and an emotion (sadness, anger, fear, joy), the model should output an intensity classification (0, 1, 2, 3). To do this, two different approaches were implemented: the first one using a knowleadge based preprocessing the second one using word embeddings (BERT). Finally, several models were used and compared to do the classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "Given a tweet and an emotion (sadness, anger, fear or joy), the proposed task is to classify it in one of four classes given its intensity: (0:no,\n",
    "1: low, 2: moderate, 3: high). The task was solved only for the English language. \n",
    "\n",
    "This task was proposed in (Mohammad et al.,2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decription of the dataset\n",
    "There are four training and test sets of labeled data â€“ one for each emotion. The data creation is described in (Mohammad and Kiritchenko, 2018).\n",
    "\n",
    "The training and test sets were merged together and then split (20/80) in order to increase the size of the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "date                                               text  \\\n2015  2018-En-01895  Sarah is a complete lunatic. How Chad is still...   \n158   2017-En-11404  @Lowetide there is no room for jokes in hockey...   \n1495  2017-En-10107  Leave it on there, rule,nimber 1 of carpet cle...   \n1884  2018-En-01755  What does Amelia want?! Sarah was v grateful  ...   \n1348  2017-En-11047  Threat factors in respect to provocation bulb ...   \n...             ...                                                ...   \n594   2017-En-10927  Sting is just too damn earnest for early morni...   \n1691  2017-En-10397  @iamsrk what's up w the gender bias? #indignan...   \n1730  2018-En-01152  @Argos_Online customer service is dreadful, ph...   \n674   2017-En-10760  Tiangong 1, China's first space laboratory, wi...   \n793   2017-En-10419  Ok but I just got called a 'White Devil' on th...   \n\n     emotion                                        level  \n2015   anger      3: high amount of anger can be inferred  \n158    anger      3: high amount of anger can be inferred  \n1495   anger      3: high amount of anger can be inferred  \n1884   anger                  0: no anger can be inferred  \n1348   anger  2: moderate amount of anger can be inferred  \n...      ...                                          ...  \n594    anger                  0: no anger can be inferred  \n1691   anger  2: moderate amount of anger can be inferred  \n1730   anger      3: high amount of anger can be inferred  \n674    anger                  0: no anger can be inferred  \n793    anger  2: moderate amount of anger can be inferred  \n\n[2089 rows x 4 columns]\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "li = []\n",
    "all_files= {'../datasets/2018-EI-oc-En-anger-dev.txt', '../datasets/EI-oc-En-anger-train.txt'}\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", header=None, skiprows=1)\n",
    "    li.append(df)\n",
    "\n",
    "dataset = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "dataset.columns = ['date', 'text', 'emotion', 'level']\n",
    "dataset= dataset.sample(frac=1)\n",
    "print (dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Approach \n",
    "Two alternatives were implemented for preprocessing: a knowleadge based approach (section 3.1.1) and one using word embeddings (BERT) (section 3.1.2). After the preprocessing, both data were run for the same models: Naive Bayes, SVM, Logistic regression, Multi-layer Perceptron classifier, Perceptron, Decision Tree and Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Preprocessing using a knowleadge based approach\n",
    "In this approach, the following processes were implemented:\n",
    "* **Tokenization**: tokens were created using Python String split() method. Other tokenization methods were tested such as NLTK word_tokenize(), but no significant improvements were observed.\n",
    "* **Lower case**: tokens were converted to lower case\n",
    "* **Lemmatization**: using NLTK WordNetLemmatizer\n",
    "* **User**: All user mentions were substituted with '@user'\n",
    "* **Emojis**: All emojis were converted to keywords, using the 'emoji' library\n",
    "* **TF-IDF**: SkLearn's TfidfVectorizer was used to perform TF-IDF\n",
    "* **Bag of Words (BoW)**: Bag of words was used but eventually substitute with TF-IDF, because of the better overall accuracy\n",
    "\n",
    "\n",
    "*Skip to section 3.1.2. and do not run the following 3 cells if you want to see the results from using word embeddings (BERT).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "corpus = []\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    tweet = row['text']\n",
    "\n",
    "    #handle users\n",
    "    tweet = re.sub('@.*', '@user', tweet) \n",
    "\n",
    "    \n",
    "    \n",
    "    tweet = tweet.tolower().split()\n",
    "    #tweet = nltk.word_tokenize(tweet)\n",
    "\n",
    "    # stemming and stop word removal\n",
    "    tweet = ' '.join([lemmatizer.lemmatize(w) for w in tweet if not w in set(stopwords.words('english'))])\n",
    "    \n",
    "\n",
    "    #tweet = nlp(tweet) # run annotation over a sentence\n",
    "    \n",
    "    \n",
    "    #emojis\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    \n",
    "    corpus.append(tweet)\n",
    "\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tfidf_vectorizer_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1500)\n",
    "#X = vectorizer.fit_transform(corpus).toarray()\n",
    "X = tfidf_vectorizer_vectors.toarray()\n",
    "y = []\n",
    "for index, row in dataset.iterrows():\n",
    "    y.append(int(row['level'][0]))\n",
    "    '''\n",
    "    if(int(row['level'][0])==3):\n",
    "        y.append(1) \n",
    "    elif(int(row['level'][0])>0):\n",
    "        y.append(1) \n",
    "    else:\n",
    "        y.append(0)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "y = np.array(y)\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(type(X[0]), y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Preprocessing using BERT\n",
    "In this approach, word embedding was used to preprocess the data. For this, BERT was chosen and the library *bert_embedding* was used because of its simplicity. The model was pre-trained in the following dataset: *book_corpus_wiki_en_cased*.\n",
    "\n",
    "*Skip to section 3.2 and do not run the following 3 cells if you want to see the results from using the knowleadge based approach described in 3.1.1.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "at32), array([ 0.61834633,  0.7299718 , -0.39379093, ...,  0.86459017,\n       -0.05611638, -0.10752675], dtype=float32), array([-0.0568656 , -1.0629956 , -0.33967328, ..., -0.8862806 ,\n        0.0664607 , -0.27232605], dtype=float32), array([-0.36749876, -0.12939635,  0.35977873, ...,  0.1538951 ,\n        0.52668357, -0.43332773], dtype=float32), array([-0.14321527,  0.28332147,  0.2589766 , ...,  0.10303332,\n        0.4076267 ,  0.8468303 ], dtype=float32), array([-0.5571995 ,  0.05310187, -0.3118394 , ...,  0.13199309,\n        0.38863054,  0.04157293], dtype=float32), array([-0.5891458 , -0.52483165, -0.51016456, ..., -0.17108464,\n       -0.32084063, -0.16606271], dtype=float32)]), (['@', 'paulsherard', '@', 'margarethaynie', 'wow', 'I', 'just', 'realized', 'how', 'much', 'of', 'a', 'sexist', 'pig'], [array([-0.37625355, -1.0494866 , -0.19031039, ..., -0.6830996 ,\n        0.10930118, -0.43802965], dtype=float32), array([-0.36003363,  0.33860287,  0.08261   , ..., -0.85542107,\n        0.35890198, -0.33992168], dtype=float32), array([-0.46083158, -0.25804037, -0.67624384, ...,  0.14986148,\n       -0.5990883 ,  0.1659715 ], dtype=float32), array([-0.08678363,  0.02670277,  0.10466385, ..., -0.25155714,\n        0.06628764, -0.13026373], dtype=float32), array([ 0.34123027, -0.67232126, -0.6452391 , ...,  0.03505829,\n        0.7451105 , -0.14968398], dtype=float32), array([-0.06168152, -0.22588529,  0.7540541 , ...,  0.6642537 ,\n        0.35891455, -0.4049543 ], dtype=float32), array([-0.38649213,  0.24818231,  0.62378   , ...,  0.43317413,\n       -0.12600991,  0.00542884], dtype=float32), array([-0.65108675,  0.7786839 ,  0.11095823, ...,  0.58418053,\n        0.15446785, -0.75000834], dtype=float32), array([-0.35466748, -0.2157244 ,  0.22031304, ...,  0.25939834,\n        1.2972759 , -0.48729062], dtype=float32), array([ 0.33637512, -0.22610396, -0.3868785 , ...,  1.1474421 ,\n        0.625366  ,  0.16397941], dtype=float32), array([ 0.08303793,  0.66782093, -0.07846786, ...,  0.13640103,\n        0.754501  ,  0.8760312 ], dtype=float32), array([0.74048007, 0.52050817, 0.19068912, ..., 0.6393863 , 1.0327108 ,\n       0.07871553], dtype=float32), array([ 0.23008579,  0.54042125,  0.33332193, ...,  1.1044422 ,\n        0.18634993, -0.23396082], dtype=float32), array([-0.10511082, -0.7897412 , -0.6711427 , ...,  0.32380703,\n       -0.08373053, -0.04019243], dtype=float32)]), (['@', 'amjadhussain73', '@', 'MehreenFaruqi', 'Don', \"'\", 't', 'waste', 'your', 'time', 'on', 'that'], [array([-0.3716305 , -1.1952612 , -0.14634031, ..., -0.77058387,\n        0.12367246, -0.30715913], dtype=float32), array([-0.47574544,  0.74252737, -0.04978082, ..., -0.07127138,\n       -0.19286653,  0.27984008], dtype=float32), array([-0.13404442, -0.3829323 , -0.49497145, ..., -0.07822011,\n       -0.4021318 ,  0.00857803], dtype=float32), array([-0.36536443,  0.70947963, -0.11599752, ...,  0.1046403 ,\n        0.16388512,  0.20562132], dtype=float32), array([ 0.9514586 , -0.21143556,  0.42052558, ..., -0.17634168,\n       -0.6885401 ,  0.29312044], dtype=float32), array([-0.3965361 ,  0.8482064 , -0.32412308, ..., -0.39787307,\n        0.35803497, -0.03460594], dtype=float32), array([ 0.7704781 , -0.9121915 ,  0.48812452, ...,  0.02187698,\n       -0.75293136, -0.45321792], dtype=float32), array([-0.3408285 ,  0.11936465, -1.4203326 , ...,  0.3554835 ,\n       -0.4008679 ,  0.28198177], dtype=float32), array([ 1.2639998 ,  0.5088093 , -0.3086242 , ..., -0.30442703,\n       -0.436639  ,  0.30351245], dtype=float32), array([ 0.79751164,  0.56329966, -0.8415112 , ..., -0.11854651,\n       -0.761024  ,  0.24511947], dtype=float32), array([ 0.36777186,  0.356439  ,  0.01178037, ..., -0.5318271 ,\n        0.31508783,  0.4688356 ], dtype=float32), array([ 0.4527015 ,  0.4433892 , -0.19252157, ...,  0.22686464,\n       -0.36304832,  0.53364486], dtype=float32)]), (['Delete', 'this', 'shit', 'Josh', 'just', 'screenshotted', 'my', 'snap', 'me'], [array([-0.67390996,  0.07428055,  0.11105759, ...,  0.14317413,\n        0.26136535, -0.0861553 ], dtype=float32), array([ 0.47979516, -0.03369541,  0.6941612 , ...,  0.38488817,\n        0.42109722,  0.79069585], dtype=float32), array([ 0.17302825, -0.09181243,  0.2044692 , ...,  0.8636232 ,\n        0.45614016, -0.31960022], dtype=float32), array([0.19051243, 0.36154297, 0.3717549 , ..., 1.624162  , 0.06757498,\n       0.3541848 ], dtype=float32), array([ 0.7989617 ,  0.4160432 ,  0.15990552, ...,  1.0240624 ,\n       -0.14372668,  0.1500066 ], dtype=float32), array([-0.08904459,  0.6193815 , -0.18684073, ...,  0.38515624,\n        0.11042907,  0.32559565], dtype=float32), array([ 0.20909955, -0.26079443,  0.6654185 , ..., -0.15221912,\n        0.32093894,  0.67233515], dtype=float32), array([ 0.6579152 , -0.25211456,  0.13916093, ..., -0.11617763,\n       -0.04104626,  0.33761233], dtype=float32), array([-0.03114741, -0.84468335, -0.2304852 , ...,  0.00758778,\n       -0.32657802,  0.39214772], dtype=float32)]), (['smh', 'customers', 'getting', 'angry', 'at', 'me', 'bc', 'i', 'aint', 'got', 'no', 'marlboro', 'lights', 'in', 'the', 'gas', 'hut'], [array([-0.39530292, -0.8386647 ,  0.1244902 , ..., -0.35470343,\n       -0.36704877, -0.25116488], dtype=float32), array([-0.05020049, -0.37479168,  0.3871549 , ...,  0.77803826,\n       -0.23576383,  0.07185177], dtype=float32), array([-0.08091457, -0.05813635, -0.08631154, ...,  0.54289067,\n        0.6956155 , -0.5404638 ], dtype=float32), array([ 0.02220137,  1.1037854 , -0.7551385 , ...,  0.09213106,\n       -0.23676525,  0.8679216 ], dtype=float32), array([-0.7307882 ,  0.49873042, -1.0106776 , ...,  1.0290456 ,\n        0.5234085 ,  0.63582444], dtype=float32), array([-0.831215  ,  0.37287953, -0.52971345, ...,  0.7540772 ,\n        0.6766081 , -1.1079104 ], dtype=float32), array([-0.58641654, -0.61374575,  0.462051  , ...,  0.09730668,\n        0.03727815,  0.5504117 ], dtype=float32), array([-0.42381287, -0.81530803, -0.04132841, ..., -0.49774012,\n       -0.08639711, -0.3463446 ], dtype=float32), array([-0.2052289 , -0.23898464,  0.30869988, ..., -0.5338966 ,\n       -0.1656066 , -0.02533727], dtype=float32), array([-0.41365975,  1.0269848 ,  0.550076  , ..., -0.0380752 ,\n        0.15639713, -0.39035124], dtype=float32), array([ 0.41793883,  0.46583128, -0.08371597, ..., -0.70497525,\n       -0.5530533 ,  0.01735424], dtype=float32), array([ 0.19918084, -0.08320011, -0.09383541, ..., -0.00064593,\n        0.17274588, -0.2805971 ], dtype=float32), array([ 0.16003232, -0.9814637 , -0.5453039 , ...,  0.22727787,\n        0.34551936,  0.44501466], dtype=float32), array([ 0.27672756,  0.7474221 , -1.0501292 , ...,  0.22769544,\n        0.485882  ,  0.63408357], dtype=float32), array([-0.43458742,  0.93629885, -0.09174491, ...,  0.69801116,\n        0.8975214 , -0.3553248 ], dtype=float32), array([ 0.1315787 , -0.02920361, -0.6925704 , ...,  0.05145627,\n        0.16899987,  0.00948246], dtype=float32), array([ 0.26021153, -0.7663691 , -0.15225925, ...,  0.33625427,\n       -0.03980695, -0.38547587], dtype=float32)]), (['Forgot', 'to', 'eat', 'dinner', 'and', 'now', 'I', \"'\", 'm', 'furious', 'with', 'everything', 'and', 'everyone'], [array([-0.15611728, -0.42766973,  0.00193889, ..., -0.44397223,\n        0.87801397, -0.38920426], dtype=float32), array([ 1.3925263 ,  1.3303751 ,  0.06094887, ...,  0.21799791,\n        0.39318463, -0.72166586], dtype=float32), array([ 0.5919382 ,  0.3272932 , -0.24595705, ...,  0.72627944,\n        0.29624346, -0.25620347], dtype=float32), array([ 0.85604507,  0.629432  , -0.29634792, ...,  0.851328  ,\n       -0.35479894, -0.49029613], dtype=float32), array([ 0.46962202, -0.55268264, -0.5561922 , ..., -0.8000239 ,\n        0.23048896, -0.9842041 ], dtype=float32), array([-0.63240916,  0.04825228,  0.16210338, ...,  0.21934009,\n        1.0105116 , -0.04082568], dtype=float32), array([-0.21680528, -0.10656573,  0.19949776, ...,  0.49591565,\n        0.1776672 , -0.836098  ], dtype=float32), array([ 0.65572506,  1.0292859 , -0.43210453, ..., -1.1041203 ,\n        0.75928044,  0.39547253], dtype=float32), array([ 0.5248727 , -0.03749647,  0.37602544, ...,  0.58550304,\n        0.13188678, -0.46496397], dtype=float32), array([ 0.28109407,  0.7692    , -0.10523848, ..., -0.08864748,\n       -0.23742658,  0.20833626], dtype=float32), array([-0.31106076,  0.72376084, -0.4732506 , ...,  0.1358111 ,\n        0.08102705, -0.24376048], dtype=float32), array([ 0.03481171,  1.0315516 , -0.60545456, ...,  0.9953365 ,\n        0.66288024, -0.73244256], dtype=float32), array([-0.1029719 ,  0.15347463, -0.6270429 , ..., -0.27290213,\n        0.23714897, -0.81723994], dtype=float32), array([ 8.1673861e-01,  4.4641671e-01, -9.1398135e-02, ...,\n        3.8656113e-01,  7.1287155e-05, -9.0163577e-01], dtype=float32)]), (['Sting', 'is', 'just', 'too', 'damn', 'earnest', 'for', 'early', 'morning', 'listening', '.'], [array([-0.26441303, -0.78487146,  0.2788645 , ..., -0.64964867,\n        0.635865  , -0.15851757], dtype=float32), array([0.0019301 , 0.24185865, 0.920254  , ..., 0.6453741 , 0.79305017,\n       0.62449473], dtype=float32), array([0.16490231, 0.46035975, 0.6203876 , ..., 0.59293056, 0.16937236,\n       0.22127315], dtype=float32), array([ 0.5332567 ,  0.07783498,  0.46288773, ..., -0.06450269,\n        0.46074304, -0.05113459], dtype=float32), array([-0.1964303 , -0.02243102, -0.01973881, ...,  0.6875504 ,\n        0.24999192, -0.25121683], dtype=float32), array([ 0.11758849,  0.09642711, -0.4192003 , ..., -0.21760592,\n        0.44810703, -0.1620138 ], dtype=float32), array([-0.33227876,  0.9001258 ,  0.47627643, ...,  0.40791723,\n        0.47044078,  0.4736634 ], dtype=float32), array([ 0.63478893,  0.84169304, -0.10893152, ...,  0.23247626,\n        1.0476005 ,  0.4678895 ], dtype=float32), array([0.59427035, 0.5767706 , 0.17086211, ..., 0.32705975, 0.35447013,\n       0.5183745 ], dtype=float32), array([ 0.14549293, -0.12262997,  0.3095727 , ..., -0.11513659,\n       -0.27887687,  0.16567251], dtype=float32), array([ 0.18321724, -0.6346112 ,  0.1718727 , ..., -0.557461  ,\n        0.7862413 , -0.58113164], dtype=float32)]), (['@', 'iamsrk', 'what', \"'\", 's', 'up', 'w', 'the', 'gender', 'bias', '?', '#', 'indignant', 'This', 'fancypants', 'saddler', \"'\"], [array([-0.3288231 , -1.0972639 , -0.20608199, ..., -0.63882715,\n       -0.04373017, -0.32193035], dtype=float32), array([-0.13555448, -0.06088987, -0.0995369 , ..., -0.26146734,\n        0.22966976, -0.39884496], dtype=float32), array([ 0.26672572, -0.6787011 ,  0.39230406, ...,  0.3670248 ,\n       -0.25075823, -0.27348465], dtype=float32), array([ 0.5797782 ,  0.01295966,  0.4863061 , ..., -1.0436125 ,\n       -0.24507858,  1.2314036 ], dtype=float32), array([ 0.35387367, -0.04861495,  0.96187216, ...,  0.24107203,\n        0.02349063,  0.05943989], dtype=float32), array([ 0.14634009,  0.8875511 , -0.357329  , ...,  0.6063134 ,\n       -0.4855583 ,  0.46833092], dtype=float32), array([-0.2197465 , -0.28309903,  0.6648613 , ...,  0.1600003 ,\n        0.10276748,  0.4829806 ], dtype=float32), array([0.3814512 , 0.66016763, 1.3369823 , ..., 0.5441005 , 0.06095153,\n       0.12970054], dtype=float32), array([-0.17603265,  0.38099393,  0.46462998, ...,  0.32569742,\n        0.48482832,  0.23994417], dtype=float32), array([-0.29509282,  0.406615  , -0.36688483, ...,  0.540423  ,\n        0.7147603 ,  0.38859177], dtype=float32), array([-0.13372259, -0.6212342 ,  0.38078088, ...,  0.67435455,\n        0.1531027 ,  0.4433843 ], dtype=float32), array([ 0.02173328, -0.05094291, -0.20519698, ...,  0.31282842,\n       -0.3211332 ,  0.19165738], dtype=float32), array([ 0.40819433,  0.311401  ,  0.49130473, ..., -0.61950654,\n        0.33476925,  0.5830104 ], dtype=float32), array([-0.6295999 , -0.2827368 , -0.06192567, ...,  0.8873374 ,\n        0.40794477,  0.41697222], dtype=float32), array([-0.18749322,  0.54961896, -0.11205418, ..., -0.87982386,\n       -0.12394562,  0.0769034 ], dtype=float32), array([-0.12431026, -0.10048374, -0.13420258, ..., -0.33201274,\n       -0.4497788 ,  0.23888892], dtype=float32), array([-0.33208525,  0.28085968,  0.1012099 , ...,  0.41365856,\n        0.24361289, -0.00827294], dtype=float32)]), (['@', 'Argos', '_', 'Online', 'customer', 'service', 'is', 'dreadful', ',', 'phone', 'bill', 'is', 'huge', 'and', 'get', 'passed', 'from', 'person', '2', 'person', 'and'], [array([-0.3765157 , -1.0126543 , -0.22359017, ..., -0.6581077 ,\n        0.05561143, -0.37075758], dtype=float32), array([-0.58197564,  0.27163637, -0.0764854 , ..., -0.38949612,\n       -0.23571265, -0.2891834 ], dtype=float32), array([-0.37177387,  0.63780814, -0.42047477, ...,  0.03116857,\n       -0.15171199, -0.47318423], dtype=float32), array([ 0.379538  ,  1.3312998 , -0.17832103, ..., -0.03053789,\n       -0.48546174,  0.5022258 ], dtype=float32), array([ 0.26744944,  0.24765833,  0.4022035 , ...,  0.68992895,\n       -0.85013264,  0.46198493], dtype=float32), array([ 0.5218811 ,  1.2496217 ,  0.09643196, ...,  0.63647366,\n       -0.5963471 ,  0.64662635], dtype=float32), array([-0.31896448,  0.5115866 , -0.97936344, ..., -0.04235879,\n       -0.32966152,  0.20041223], dtype=float32), array([-0.6671559 ,  0.19432996, -0.83548117, ..., -0.27514505,\n       -0.40179753, -0.16441736], dtype=float32), array([-0.73130095, -0.19047005, -1.0948921 , ..., -0.55978644,\n        0.28537512,  0.16689508], dtype=float32), array([ 0.6456111 ,  1.335027  , -0.0792077 , ...,  0.42342013,\n       -0.8154967 ,  0.41482878], dtype=float32), array([ 0.2783565 , -0.38899902, -0.03992258, ...,  0.3296169 ,\n        0.0403662 ,  0.11246873], dtype=float32), array([-0.4794113 ,  0.16173705, -0.67760074, ..., -0.6131251 ,\n       -0.38243097,  0.20297591], dtype=float32), array([ 0.12413283,  0.22388272,  0.13783333, ..., -0.16135456,\n       -0.50952935, -0.1569629 ], dtype=float32), array([-0.25974303, -0.01803836, -1.1181192 , ..., -0.63236684,\n       -0.41216215,  0.21174055], dtype=float32), array([-0.38059783,  0.70204425, -0.3396891 , ..., -0.14174859,\n       -0.4290987 ,  0.48142284], dtype=float32), array([-0.23034689, -0.05121302,  0.11231492, ...,  0.03831039,\n       -0.67598397,  0.07914928], dtype=float32), array([-0.4293385 , -0.35340333,  0.04910562, ..., -0.1235847 ,\n       -0.6812128 ,  0.74391055], dtype=float32), array([ 0.59693563,  0.7657121 , -0.49973163, ..., -0.32420218,\n       -0.29801202, -0.12200601], dtype=float32), array([ 0.4620999 ,  0.28427407,  0.20252627, ..., -0.28792453,\n        0.08493473,  0.09787142], dtype=float32), array([ 0.61409074,  0.5611971 , -0.28681463, ..., -0.4919857 ,\n       -0.25515294,  0.07352208], dtype=float32), array([-0.36427784, -0.60790867, -0.32622424, ..., -0.756402  ,\n        0.04241666, -0.19144338], dtype=float32)]), (['Tiangong', '1', ',', 'China', \"'\", 's', 'first', 'space', 'laboratory', ',', 'will', 'come', 'to', 'a', 'fiery', 'end', 'in', 'late', '2017', '.', 'The'], [array([-0.6162713 , -0.5752323 ,  0.11142313, ..., -0.38718113,\n        0.24576879, -0.2894499 ], dtype=float32), array([ 0.56026834,  1.0238428 ,  0.4864141 , ...,  0.42684227,\n       -0.13360935, -0.06087889], dtype=float32), array([-0.48970914, -0.07145184,  0.72076106, ...,  0.7792186 ,\n        0.4295757 , -0.29960167], dtype=float32), array([-1.0711526 ,  0.08299085,  0.23121834, ...,  0.48798126,\n        0.3393423 ,  0.13179168], dtype=float32), array([-0.83886915,  0.8628067 ,  1.8368782 , ..., -0.81848496,\n        0.09577021,  0.31549406], dtype=float32), array([-0.6504736 ,  0.93534374,  2.274401  , ..., -0.5000813 ,\n        0.23244175,  0.6054568 ], dtype=float32), array([-0.25946733,  1.2664897 ,  0.4514983 , ..., -0.22642203,\n        0.11237562,  0.4714293 ], dtype=float32), array([-0.00566512,  0.7387797 , -0.10900507, ...,  0.59971017,\n       -0.5624523 ,  0.3142218 ], dtype=float32), array([-0.59085035, -0.19909261,  0.12850323, ...,  0.07323422,\n       -0.26401475,  0.05817246], dtype=float32), array([-0.71805245, -0.08462648,  0.7412958 , ...,  0.6353296 ,\n       -0.494376  ,  0.05935457], dtype=float32), array([-0.4624927 ,  1.1281554 ,  0.39443424, ...,  1.0596583 ,\n       -0.9415807 ,  0.570365  ], dtype=float32), array([-0.2439304 ,  0.91423035,  0.34210956, ...,  0.986944  ,\n       -0.14941247,  0.3549981 ], dtype=float32), array([-0.13160607, -0.0623446 ,  0.5941219 , ..., -1.1123812 ,\n       -0.795648  ,  1.387784  ], dtype=float32), array([-0.14054811, -0.06867146,  0.5899048 , ..., -1.1121938 ,\n       -0.7942625 ,  1.3886465 ], dtype=float32), array([-0.06015794,  0.20318581,  0.11982732, ..., -0.08538383,\n       -0.40258324,  0.44676054], dtype=float32), array([ 0.01985116,  0.02941421, -0.7524783 , ...,  0.76622725,\n       -0.02163714, -0.1474656 ], dtype=float32), array([ 0.46329945,  0.19346458,  0.7031223 , ...,  0.55662847,\n       -0.6507453 ,  1.2175143 ], dtype=float32), array([ 1.078854  ,  0.52387303,  0.48484275, ..., -0.16948026,\n       -0.07025235,  0.9684374 ], dtype=float32), array([-0.1562913 ,  0.5819873 ,  0.22341889, ...,  0.17274171,\n       -0.30619588,  0.80953604], dtype=float32), array([-0.3806349 , -0.65493065, -0.37870526, ..., -0.73958445,\n       -0.09974446,  0.03541646], dtype=float32), array([-0.25099695,  0.45024684,  0.16907647, ...,  0.74190736,\n       -0.2791392 ,  0.1889593 ], dtype=float32)]), (['Ok', 'but', 'I', 'just', 'got', 'called', 'a', \"'\", 'White', 'Devil', \"'\", 'on', 'the', 'train', 'and', 'I', 'didnt', 'know', 'whether', 'to', 'laugh', 'or'], [array([-0.31519938, -1.0777855 ,  0.12962648, ..., -0.80244017,\n        0.2349518 , -0.3648063 ], dtype=float32), array([ 0.13666244, -0.7502239 , -0.18705374, ...,  0.23031211,\n       -0.23310465, -0.28288192], dtype=float32), array([-0.23794998, -0.45539194,  0.26063693, ...,  0.29153696,\n        0.79030216, -1.0728395 ], dtype=float32), array([ 0.44812155,  0.38861388, -0.164623  , ...,  0.44806528,\n       -0.08435556,  0.1722749 ], dtype=float32), array([ 0.35681522,  0.12550484,  0.09022946, ..., -0.08006291,\n        0.9224915 , -0.37345356], dtype=float32), array([-0.4808169 ,  0.05702504, -0.07099272, ..., -0.10589671,\n        0.37152538,  0.18420386], dtype=float32), array([ 0.6703322 , -0.31618965, -0.31202227, ..., -0.42088723,\n        1.1880246 ,  0.44948006], dtype=float32), array([-0.07335126, -0.14356385,  0.22799361, ..., -0.17292874,\n        0.43922895, -0.0748717 ], dtype=float32), array([-0.11013354, -0.29246214,  0.19080797, ..., -0.7167671 ,\n        0.01250147,  0.2512607 ], dtype=float32), array([-0.18546152, -0.14151046, -0.5881357 , ...,  0.2178379 ,\n       -0.04699601, -0.30252624], dtype=float32), array([-0.33813736, -0.21835098, -0.11919142, ...,  0.3630495 ,\n        0.12049618, -0.42037237], dtype=float32), array([ 0.3139404 ,  0.025957  ,  0.06480145, ...,  0.6287096 ,\n        0.03843804, -0.29288995], dtype=float32), array([ 0.76157534,  0.2804863 ,  0.2903467 , ...,  0.3271419 ,\n        0.83436936, -0.17078285], dtype=float32), array([ 0.3247593 ,  0.33686733,  0.17841071, ..., -0.29786837,\n       -0.12027349,  0.17393282], dtype=float32), array([ 0.55186343, -0.3721808 , -1.0906541 , ..., -0.17014647,\n        0.89802045, -0.17573984], dtype=float32), array([ 0.11672561, -0.48262545,  0.01519929, ...,  0.00191208,\n        0.7361306 , -0.8129567 ], dtype=float32), array([ 0.01611194, -0.03314511, -0.00591258, ..., -0.21761855,\n       -0.1872018 , -0.18120316], dtype=float32), array([ 0.41330105,  0.87979054, -0.10972932, ...,  0.5476202 ,\n       -0.2603435 ,  0.06915471], dtype=float32), array([ 0.6791423 ,  0.16056874,  0.01349826, ...,  0.7844637 ,\n        0.26655707, -0.32937002], dtype=float32), array([ 0.32843268,  1.1429713 ,  0.39119512, ..., -0.23226848,\n        0.5135927 , -0.6503465 ], dtype=float32), array([-0.06322609,  0.11859696, -0.6497051 , ...,  0.28250948,\n       -0.09952912,  0.07326093], dtype=float32), array([ 0.6245629 , -0.46241012, -0.20300686, ...,  1.1223373 ,\n        0.6097962 , -0.4086532 ], dtype=float32)])]\n"
    }
   ],
   "source": [
    "from bert_embedding import BertEmbedding\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import mxnet as mx\n",
    "\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    tweet = row['text']\n",
    "    corpus.append(tweet)\n",
    "\n",
    "bert = BertEmbedding(model='bert_24_1024_16', dataset_name='book_corpus_wiki_en_cased')\n",
    "results = bert(corpus)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "averaged = []\n",
    "for sent in results:\n",
    "    averaged.append(np.mean(sent[1], axis = 0, dtype=np.float64))\n",
    "\n",
    "corpus=averaged\n",
    "X=np.array(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1500)\n",
    "y = []\n",
    "for index, row in dataset.iterrows():\n",
    "    y.append(int(row['level'][0]))\n",
    "    '''\n",
    "    if(int(row['level'][0])==3):\n",
    "        y.append(1) \n",
    "    elif(int(row['level'][0])>0):\n",
    "        y.append(1) \n",
    "    else:\n",
    "        y.append(0)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "y = np.array(y)\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(type(X[0]), y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Classification\n",
    "After the preprocessing, the data was run for the following models (All from SkLearn). \n",
    "* Naive Bayes \n",
    "* Support Vector Machine(SVM)\n",
    "* Logistic regression\n",
    "* Multi-layer Perceptron classifier\n",
    "* Perceptron\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "\n",
    "To evaluate and compare them, Accuracy, Precision, Recall and F1 were measured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1671, 1024) (1671,)\n(418, 1024) (418,)\n"
    }
   ],
   "source": [
    "# Split dataset into training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[56 36 16 19]\n [30 15 25  7]\n [21 31 36 29]\n [12  9 11 65]]\nAccuracy:  0.41148325358851673\nPrecision:  0.4135462119735973\nRecall:  0.41148325358851673\nF1:  0.40855124453720665\n"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[84  1 35  7]\n [39  1 36  1]\n [41  1 61 14]\n [ 8  0 34 55]]\nAccuracy:  0.48086124401913877\nPrecision:  0.47839620255968635\nRecall:  0.48086124401913877\nF1:  0.4426862707090956\n"
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[73 23 19 12]\n [28 13 32  4]\n [34 25 38 20]\n [ 6  6 21 64]]\nAccuracy:  0.44976076555023925\nPrecision:  0.438254194677727\nRecall:  0.44976076555023925\nF1:  0.44326921488042503\n"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[40 23 39 25]\n [15 15 38  9]\n [16 22 45 34]\n [ 3  9 22 63]]\nAccuracy:  0.38995215311004783\nPrecision:  0.40334721063460127\nRecall:  0.38995215311004783\nF1:  0.3835389287892652\n"
    }
   ],
   "source": [
    "# SGDC classifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "classifier = SGDClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[78 22 17 10]\n [30 20 25  2]\n [35 23 41 18]\n [ 7  8 19 63]]\nAccuracy:  0.48325358851674644\nPrecision:  0.47816980764358763\nRecall:  0.48325358851674644\nF1:  0.47892663637526395\n"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifier = MLPClassifier(random_state=1, max_iter=300)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[82 24  8 13]\n [41 15 12  9]\n [46 33 16 22]\n [ 9 11 11 66]]\nAccuracy:  0.42822966507177035\nPrecision:  0.4077776743114188\nRecall:  0.42822966507177035\nF1:  0.40050326637946165\n"
    }
   ],
   "source": [
    "# Perceptron\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "classifier = Perceptron() \n",
    "classifier.fit(X_train, y_train) \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred)) \n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted')) \n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted')) \n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[51 30 32 14]\n [29 20 16 12]\n [30 24 42 21]\n [15 12 25 45]]\nAccuracy:  0.37799043062200954\nPrecision:  0.3825336452170043\nRecall:  0.37799043062200954\nF1:  0.38003113057949217\n"
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[88 10 22  7]\n [41  3 28  5]\n [49  1 55 12]\n [14  4 28 51]]\nAccuracy:  0.47129186602870815\nPrecision:  0.44350508448153875\nRecall:  0.47129186602870815\nF1:  0.4400367924065352\n"
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-dde29e7eb427>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^a-zA-Z]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/python37/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m                 \"string object received.\")\n\u001b[0;32m-> 1267\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/python37/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "#Test here with your input\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "tweet = input(\"Enter tweet: \")\n",
    "tweet = re.sub('[^a-zA-Z]', ' ', tweet).split()\n",
    "tweet = ' '.join([ps.stem(w) for w in tweet])\n",
    "X = vectorizer.transform([tweet]).toarray()\n",
    "\n",
    "print(X.shape)\n",
    "print(X)\n",
    "\n",
    "print(\"Sentiment level: \", classifier.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Experimental evaluation\n",
    "\n",
    "The following values for the accuracy were obtain as the median of 3 runs for each model.\n",
    "\n",
    "#### 4.1 Joy\n",
    "\n",
    "| Models | Naive Bayes | SVM | Logistic Regression | MLPClassifier | Perceptron | Decision tree | Random forest |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Knowleadge | 0.346 | **0.419** | 0.408 | 0.369 | 0.397 | 0.390 | 0.408 |\n",
    "| BERT | 0.471 | 0.440 | 0.424 | 0.458 | 0.400 | 0.372 | **0.476** |\n",
    "\n",
    "#### 4.2 Sadness\n",
    "\n",
    "| Models | Naive Bayes | SVM | Logistic Regression | MLPClassifier | Perceptron | Decision tree | Random forest |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Knowleadge | 0.326 | 0.436 | 0.471 | 0.443 | **0.474** | 0.463 | **0.474** |\n",
    "| BERT | 0.409 | **0.489** | 0.437 | 0.472 | 0.409 | 0.319 | 0.432 |\n",
    "\n",
    "#### 4.3 Anger\n",
    "\n",
    "| Models | Naive Bayes | SVM | Logistic Regression | MLPClassifier | Perceptron | Decision tree | Random forest |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Knowleadge | 0.284 | 0.430 | **0.447** | 0.411 | 0.406 | 0.409 | **0.447** |\n",
    "| BERT | 0.411 | **0.480** | 0.450 | 0.483 | 0.428 | 0.378 | 0.443 |\n",
    "\n",
    "#### 4.4 Fear\n",
    "\n",
    "| Models | Naive Bayes | SVM | Logistic Regression | MLPClassifier | Perceptron | Decision tree | Random forest |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Knowleadge | 0.319 | 0.677 | 0.674 | 0.633 | 0.686 | 0.640 | **0.691** |\n",
    "| BERT | 0.539 | **0.681** | 0.621 | 0.671 | 0.639 | 0.533 | 0.665 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Priban, Pavel & Hercig, TomÃ¡Å¡ & Lenc, Ladislav. (2018). UWB at SemEval-2018 Task 1: Emotion Intensity Detection in Tweets. 133-140. 10.18653/v1/S18-1018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('python37': venv)",
   "language": "python",
   "name": "python36964bitpython37venvb1b630360d0f4ea08bfe0f6fcf349387"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
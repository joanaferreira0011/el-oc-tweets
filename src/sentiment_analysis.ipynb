{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza    \n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "#dataset = pd.read_csv('../datasets/2018-EI-oc-En-joy-dev.txt', sep=\"\\t\", header=None, skiprows=1)\n",
    "li = []\n",
    "all_files= {'../datasets/2018-EI-oc-En-joy-dev.txt', '../datasets/2018-EI-oc-En-sadness-dev.txt', '../datasets/2018-EI-oc-En-anger-dev.txt', '../datasets/2018-EI-oc-En-fear-dev.txt'}\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", header=None, skiprows=1)\n",
    "    li.append(df)\n",
    "\n",
    "dataset = pd.concat(li, axis=0, ignore_index=True)\n",
    "dataset.columns = ['date', 'text', 'emotion', 'level']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "\n",
    "corpus = []\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    tweet = row['text']\n",
    "\n",
    "    #handle users\n",
    "    tweet = re.sub('@.*', '@user', tweet) \n",
    "\n",
    "    \n",
    "    \n",
    "    tweet = tweet.split()\n",
    "    #tweet = nltk.word_tokenize(tweet)\n",
    "\n",
    "    # stemming and stop word removal\n",
    "    tweet = ' '.join([ps.stem(w) for w in tweet if not w in set(stopwords.words('english'))])\n",
    "    \n",
    "\n",
    "    #tweet = nlp(tweet) # run annotation over a sentence\n",
    "    \n",
    "    \n",
    "    #emojis\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    \n",
    "    corpus.append(tweet)\n",
    "\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tfidf_vectorizer_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1500)\n",
    "#X = vectorizer.fit_transform(corpus).toarray()\n",
    "X = tfidf_vectorizer_vectors.toarray()\n",
    "y = []\n",
    "for index, row in dataset.iterrows():\n",
    "    y.append(int(row['level'][0]))\n",
    "y = np.array(y)\n",
    "#print(vectorizer.get_feature_names())\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "classifier = Perceptron() \n",
    "classifier.fit(X_train, y_train) \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred)) \n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted')) \n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted')) \n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "tweet = input(\"Enter tweet: \")\n",
    "tweet = re.sub('[^a-zA-Z]', ' ', rev).split()\n",
    "tweet = ' '.join([ps.stem(w) for w in tweet])\n",
    "X = vectorizer.transform([tweet]).toarray()\n",
    "\n",
    "print(X.shape)\n",
    "print(X)\n",
    "\n",
    "print(\"Sentiment level: \", classifier.predict(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('python37': venv)",
   "language": "python",
   "name": "python36964bitpython37venvb1b630360d0f4ea08bfe0f6fcf349387"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}